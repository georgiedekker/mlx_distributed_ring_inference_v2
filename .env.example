# ===================================
# MLX Distributed Inference Configuration
# ===================================
# Copy this file to .env and adjust values as needed
# All values shown are the defaults used when not specified

# ===================================
# Model Configuration
# ===================================
# HuggingFace model repository
MODEL_REPO=mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit

# Model cache directory (${USER} will be replaced with current username)
# Leave empty to use default HuggingFace cache location
MODEL_CACHE_DIR=/Users/${USER}/.cache/huggingface/hub

# ===================================
# Network Configuration
# ===================================
# API server host (main node)
API_HOST=192.168.5.1

# API server port
API_PORT=8100

# Worker hosts (comma-separated for multiple workers)
# Format: host1,host2,host3
WORKER_HOSTS=192.168.5.2

# SSH connection string for workers (comma-separated)
# Format: user@host or just host for localhost
WORKER_SSH=mini2@192.168.5.2

# ===================================
# KV-Cache Configuration
# ===================================
# Maximum KV-cache size (leave empty for no limit)
KV_CACHE_MAX_SIZE=

# Reserved memory for KV-cache in MB
KV_CACHE_RESERVED_MEMORY_MB=2048

# Maximum sequence length
MAX_SEQUENCE_LENGTH=4096

# ===================================
# Performance Tuning
# ===================================
# Maximum prompt length in bytes for broadcast
MAX_PROMPT_LEN_BYTES=4096

# Request timeout in seconds
REQUEST_TIMEOUT_SECONDS=120

# Poll interval in seconds (for checking requests)
POLL_INTERVAL_SECONDS=0.1

# Default max tokens for generation
DEFAULT_MAX_TOKENS=50

# ===================================
# Distributed Settings
# ===================================
# Number of distributed devices/workers
NUM_DEVICES=2

# MLX distributed backend (ring, nccl, etc.)
DISTRIBUTED_BACKEND=ring

# ===================================
# File Paths
# ===================================
# Temporary file locations for IPC
REQUEST_FILE_PATH=/tmp/mlx_request.json
RESPONSE_FILE_PATH=/tmp/mlx_response.json

# Log file locations
SERVER_LOG_PATH=server.log
API_LOG_PATH=api.log

# ===================================
# System Configuration
# ===================================
# File descriptor limits (soft, hard)
FILE_DESCRIPTOR_SOFT_LIMIT=2048
FILE_DESCRIPTOR_HARD_LIMIT=4096

# Model loading wait time in seconds
MODEL_LOAD_WAIT_SECONDS=15

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
